{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUTmpUGQU6ZyB7lbbhUa0h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ykWGcvahYRzD"},"outputs":[],"source":["# !pip install transformers\n","# !pip install keras_preprocessing\n","!pip install transformers datasets evaluate accelerate\n","# !pip install emoji\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/Populism\n","\n","\n","from huggingface_hub import notebook_login\n","notebook_login()\n","import pandas as pd\n","from datasets import Dataset, DatasetDict\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BertForSequenceClassification\n","import evaluate\n","import numpy as np\n","\n","# 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"]},{"cell_type":"code","source":["##################################################################################################\n","# If you would like to fine-tune a pre-trained language model\n","##################################################################################################\n","\n","# # prepare dataset\n","# df_train: dataframe of training data (or just an entire dataset that has not been split to train/validate)\n","# df_val: dataframe of validation data if already split into train/validate\n","# need_split: a boolean variable indicating whether the data needs to be split into train/validate\n","# random_seed: for reproducibility\n","\n","def prep_data(df_train, df_val=None, need_split=True, random_seed = 42):\n","  if need_split:\n","    df_train, df_val = train_test_split(df_train, random_state = random_seed, test_size=0.2) # returns 2 dataframes\n","\n","  training = pd.DataFrame({\n","      \"label\": list(df_train['peoplecentric']),\n","      \"text\": list(df_train['Message']),\n","      # \"id\": list(df_train['id'])\n","  })\n","  train_dataset = Dataset.from_dict(training)\n","  validation = pd.DataFrame({\n","      \"label\": list(df_val['peoplecentric']),\n","      \"text\": list(df_val['Message']),\n","      # \"id\": list(df_val['id'])\n","  })\n","  val_dataset = Dataset.from_dict(validation)\n","  my_dataset_dict = DatasetDict({\"train\":train_dataset,\"val\":val_dataset})\n","\n","  return my_dataset_dict"],"metadata":{"id":"wwWBarxxYUqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('./merged_labeled_data/Germanic_train.csv', index_col=0)\n","df['peoplecentric'] = df['peoplecentric'].astype(int)\n","df = df[df['antielite']==1]\n","# df = pd.read_csv('./merged_labeled_data/Germanic_train.csv')\n","data = prep_data(df, need_split = True)\n","data"],"metadata":{"id":"wTOfHc7aYWpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model_dir = \"./off_the_shelf_models/HateBERT\" # \"bert-base-uncased\"\n","\n","# model_dir = \"google-bert/bert-base-multilingual-cased\" # \"bigscience/bloom\"\n","model_dir = \"bigscience/bloom\"\n","tokenizer = AutoTokenizer.from_pretrained(model_dir) # can be changed to any model hosted on Huggingface\n","# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)\n","\n","tokenized_data = data.map(preprocess_function, batched=True)\n","# tokenizer(my_dataset_dict['test'][0][\"text\"], truncation=True)"],"metadata":{"id":"zPHwdDTDYZhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    model_dir,\n","    num_labels = 2 # increase for multi-class tasks\n",")"],"metadata":{"id":"NmBTq2zYYa-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import evaluate\n","# accuracy = evaluate.load(\"accuracy\")\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(labels, predictions)\n","    f1 = f1_score(labels, predictions, average='binary') # report 1's f1\n","    return {\n","        'accuracy': accuracy,\n","        'f1': f1\n","    }\n","    # return accuracy.compute(predictions=predictions, references=labels)\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=\"germanic_peoplecentric_bloom\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_steps=100,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"train\"],\n","    eval_dataset=tokenized_data[\"val\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"i31wEE-OYdRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train() # fff78532048001cab923f927722da80c829ec7a5\n","trainer.push_to_hub()"],"metadata":{"id":"TnFQ_2NpYf5B"},"execution_count":null,"outputs":[]}]}